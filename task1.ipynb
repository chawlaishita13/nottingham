{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a464318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ABC files...\n",
      "Creating datasets...\n",
      "Saving preprocessed data...\n",
      "Preprocessing complete!\n",
      "Vocabulary size: 544\n",
      "Number of training sequences: 11 (78.6%)\n",
      "Number of validation sequences: 1 (7.1%)\n",
      "Number of test sequences: 2 (14.3%)\n",
      "Training dataset shape: torch.Size([11, 512])\n",
      "Validation dataset shape: torch.Size([1, 512])\n",
      "Test dataset shape: torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from music21 import converter, note, chord\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Tuple\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "class MusicPreprocessor:\n",
    "    def __init__(self, abc_dir: str):\n",
    "        self.abc_dir = abc_dir\n",
    "        self.token2idx = {}\n",
    "        self.idx2token = {}\n",
    "        self.vocab_size = 0\n",
    "        self.special_tokens = {\n",
    "            'PAD': '[PAD]',\n",
    "            'START': '[START]',\n",
    "            'END': '[END]',\n",
    "            'UNK': '[UNK]'\n",
    "        }\n",
    "        self._initialize_special_tokens()\n",
    "\n",
    "    def _initialize_special_tokens(self):\n",
    "        \"\"\"Initialize special tokens in the vocabulary\"\"\"\n",
    "        for token in self.special_tokens.values():\n",
    "            self._add_to_vocab(token)\n",
    "\n",
    "    def _add_to_vocab(self, token: str):\n",
    "        \"\"\"Add a token to the vocabulary if it doesn't exist\"\"\"\n",
    "        if token not in self.token2idx:\n",
    "            self.token2idx[token] = self.vocab_size\n",
    "            self.idx2token[self.vocab_size] = token\n",
    "            self.vocab_size += 1\n",
    "\n",
    "    def _tokenize_score(self, score) -> List[str]:\n",
    "        \"\"\"Convert a music21 score into a sequence of tokens\"\"\"\n",
    "        tokens = []\n",
    "        for element in score.recurse().notes:\n",
    "            if isinstance(element, note.Note):\n",
    "                # Format: NOTE_pitch_duration\n",
    "                token = f\"NOTE_{element.pitch.nameWithOctave}_{element.quarterLength}\"\n",
    "                tokens.append(token)\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                # Format: CHORD_pitch1-pitch2-pitch3_duration\n",
    "                pitches = \"-\".join(n.pitch.nameWithOctave for n in element.notes)\n",
    "                token = f\"CHORD_{pitches}_{element.quarterLength}\"\n",
    "                tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    def process_files(self, train_split: float = 0.8, val_split: float = 0.1) -> Tuple[List[List[int]], List[List[int]], List[List[int]]]:\n",
    "        \"\"\"Process all ABC files and return tokenized sequences split into train, validation, and test sets\"\"\"\n",
    "        all_sequences = []\n",
    "        \n",
    "        # Process all files first\n",
    "        for filename in os.listdir(self.abc_dir):\n",
    "            if filename.endswith('.abc'):\n",
    "                file_path = os.path.join(self.abc_dir, filename)\n",
    "                try:\n",
    "                    # Parse the ABC file\n",
    "                    score = converter.parse(file_path)\n",
    "                    \n",
    "                    # Tokenize the score\n",
    "                    tokens = self._tokenize_score(score)\n",
    "                    \n",
    "                    # Add tokens to vocabulary\n",
    "                    for token in tokens:\n",
    "                        self._add_to_vocab(token)\n",
    "                    \n",
    "                    # Create sequence with start and end tokens\n",
    "                    sequence = [self.token2idx[self.special_tokens['START']]]\n",
    "                    sequence.extend(self.token2idx[token] for token in tokens)\n",
    "                    sequence.append(self.token2idx[self.special_tokens['END']])\n",
    "                    \n",
    "                    all_sequences.append(sequence)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "        \n",
    "        # Shuffle sequences\n",
    "        random.shuffle(all_sequences)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        total_size = len(all_sequences)\n",
    "        train_size = int(total_size * train_split)\n",
    "        val_size = int(total_size * val_split)\n",
    "        \n",
    "        # Split into train, validation, and test\n",
    "        train_sequences = all_sequences[:train_size]\n",
    "        val_sequences = all_sequences[train_size:train_size + val_size]\n",
    "        test_sequences = all_sequences[train_size + val_size:]\n",
    "        \n",
    "        return train_sequences, val_sequences, test_sequences\n",
    "\n",
    "    def save_vocab(self, save_path: str):\n",
    "        \"\"\"Save vocabulary to a file\"\"\"\n",
    "        vocab_data = {\n",
    "            'token2idx': self.token2idx,\n",
    "            'idx2token': self.idx2token,\n",
    "            'vocab_size': self.vocab_size\n",
    "        }\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(vocab_data, f)\n",
    "\n",
    "def create_dataset(sequences: List[List[int]], max_len: int = 512) -> torch.Tensor:\n",
    "    \"\"\"Create padded dataset from sequences\"\"\"\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        if len(seq) > max_len:\n",
    "            # Truncate sequence if too long\n",
    "            padded_seq = seq[:max_len]\n",
    "        else:\n",
    "            # Pad sequence if too short\n",
    "            padded_seq = seq + [0] * (max_len - len(seq))\n",
    "        padded_sequences.append(padded_seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    abc_dir = \"nottingham-dataset/ABC_cleaned\"\n",
    "    preprocessor = MusicPreprocessor(abc_dir)\n",
    "    \n",
    "    # Process all files and split into train/val/test\n",
    "    print(\"Processing ABC files...\")\n",
    "    train_sequences, val_sequences, test_sequences = preprocessor.process_files(train_split=0.8, val_split=0.1)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = create_dataset(train_sequences)\n",
    "    val_dataset = create_dataset(val_sequences)\n",
    "    test_dataset = create_dataset(test_sequences)\n",
    "    \n",
    "    # Save vocabulary and datasets\n",
    "    print(\"Saving preprocessed data...\")\n",
    "    preprocessor.save_vocab(\"vocab.pkl\")\n",
    "    torch.save({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset,\n",
    "        'test': test_dataset\n",
    "    }, \"dataset.pt\")\n",
    "    \n",
    "    print(f\"Preprocessing complete!\")\n",
    "    print(f\"Vocabulary size: {preprocessor.vocab_size}\")\n",
    "    print(f\"Number of training sequences: {len(train_sequences)} ({len(train_sequences)/len(train_sequences+val_sequences+test_sequences)*100:.1f}%)\")\n",
    "    print(f\"Number of validation sequences: {len(val_sequences)} ({len(val_sequences)/len(train_sequences+val_sequences+test_sequences)*100:.1f}%)\")\n",
    "    print(f\"Number of test sequences: {len(test_sequences)} ({len(test_sequences)/len(train_sequences+val_sequences+test_sequences)*100:.1f}%)\")\n",
    "    print(f\"Training dataset shape: {train_dataset.shape}\")\n",
    "    print(f\"Validation dataset shape: {val_dataset.shape}\")\n",
    "    print(f\"Test dataset shape: {test_dataset.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0a209f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 512):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        return x + self.pe[:x.size(0)]\n",
    "\n",
    "# class MusicTransformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         vocab_size: int,\n",
    "#         d_model: int = 512,\n",
    "#         nhead: int = 8,\n",
    "#         num_encoder_layers: int = 6,\n",
    "#         num_decoder_layers: int = 6,\n",
    "#         dim_feedforward: int = 2048,\n",
    "#         dropout: float = 0.1,\n",
    "#         max_len: int = 512\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "#         self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "#         self.transformer = nn.Transformer(\n",
    "#             d_model=d_model,\n",
    "#             nhead=nhead,\n",
    "#             num_encoder_layers=num_encoder_layers,\n",
    "#             num_decoder_layers=num_decoder_layers,\n",
    "#             dim_feedforward=dim_feedforward,\n",
    "#             dropout=dropout,\n",
    "#             batch_first=True\n",
    "#         )\n",
    "        \n",
    "#         self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "#         self.d_model = d_model\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        d_model: int = 512,\n",
    "        nhead: int = 8,\n",
    "        num_layers: int = 12,  # Increased depth\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        max_len: int = 512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Use decoder-only architecture\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerDecoder(decoder_layer, num_layers)\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    # def forward(self, x, mask=None):\n",
    "    #     # Embed and add positional encoding\n",
    "    #     x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "    #     x = self.pos_encoder(x)\n",
    "        \n",
    "    #     # Create causal mask if not provided\n",
    "    #     if mask is None:\n",
    "    #         mask = self.generate_square_subsequent_mask(x.size(1)).to(x.device)\n",
    "        \n",
    "    #     # For decoder-only, we use the same sequence as both input and memory\n",
    "    #     output = self.transformer(x, x, tgt_mask=mask)\n",
    "    #     return self.fc_out(output)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: torch.Tensor,\n",
    "        tgt: torch.Tensor,\n",
    "        src_mask: torch.Tensor = None,\n",
    "        tgt_mask: torch.Tensor = None,\n",
    "        memory_mask: torch.Tensor = None,\n",
    "        src_key_padding_mask: torch.Tensor = None,\n",
    "        tgt_key_padding_mask: torch.Tensor = None,\n",
    "        memory_key_padding_mask: torch.Tensor = None\n",
    "    ) -> torch.Tensor:\n",
    "        # Embed the source and target sequences\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        # Pass through transformer\n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary size\n",
    "        return self.fc_out(output)\n",
    "    def generate(\n",
    "        self,\n",
    "        start_token: int = 1,\n",
    "        max_len: int = 512,\n",
    "        temperature: float = 1.2,  # Higher temperature for more variety\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.95,\n",
    "        device: torch.device = None\n",
    "    ) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        if device is None:\n",
    "            device = next(self.parameters()).device\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Start with just the start token\n",
    "            sequence = torch.tensor([[start_token]], device=device)\n",
    "            \n",
    "            for _ in range(max_len - 1):\n",
    "                # Get predictions for next token\n",
    "                logits = self.forward(sequence)\n",
    "                next_token_logits = logits[0, -1, :] / temperature\n",
    "                \n",
    "                # Apply top-k filtering\n",
    "                if top_k > 0:\n",
    "                    top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k)\n",
    "                    next_token_logits = torch.full_like(next_token_logits, float('-inf'))\n",
    "                    next_token_logits.scatter_(0, top_k_indices, top_k_logits)\n",
    "                \n",
    "                # Apply top-p (nucleus) filtering\n",
    "                if top_p < 1.0:\n",
    "                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
    "                    sorted_indices_to_remove[0] = 0\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(0, sorted_indices, sorted_indices_to_remove)\n",
    "                    next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).unsqueeze(0)\n",
    "                \n",
    "                # Append to sequence\n",
    "                sequence = torch.cat([sequence, next_token], dim=1)\n",
    "                \n",
    "                # Stop if we hit the end token\n",
    "                if next_token.item() == 2:  # Assuming 2 is END token\n",
    "                    break\n",
    "                    \n",
    "            return sequence\n",
    "\n",
    "    # def generate(\n",
    "    #     self,\n",
    "    #     src: torch.Tensor,\n",
    "    #     max_len: int = 512,\n",
    "    #     temperature: float = 1.0,\n",
    "    #     top_k: int = 0,\n",
    "    #     top_p: float = 0.9\n",
    "    # ) -> torch.Tensor:\n",
    "    #     self.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         batch_size = src.shape[0]\n",
    "            \n",
    "    #         # Start with START token\n",
    "    #         dec_input = torch.ones(batch_size, 1).long().to(src.device)\n",
    "            \n",
    "    #         for _ in range(max_len - 1):\n",
    "    #             # Create masks\n",
    "    #             src_mask = torch.zeros((src.shape[1], src.shape[1])).to(src.device)\n",
    "    #             tgt_mask = self.generate_square_subsequent_mask(dec_input.shape[1]).to(src.device)\n",
    "                \n",
    "    #             # Get next token probabilities\n",
    "    #             out = self.forward(\n",
    "    #                 src,\n",
    "    #                 dec_input,\n",
    "    #                 src_mask=src_mask,\n",
    "    #                 tgt_mask=tgt_mask\n",
    "    #             )\n",
    "                \n",
    "    #             # Get probabilities for next token\n",
    "    #             next_token_logits = out[:, -1, :] / temperature\n",
    "                \n",
    "    #             # Apply top-k filtering\n",
    "    #             if top_k > 0:\n",
    "    #                 indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "    #                 next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "    #             # Apply top-p (nucleus) filtering\n",
    "    #             if top_p < 1.0:\n",
    "    #                 sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "    #                 cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    #                 sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    #                 sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    #                 sorted_indices_to_remove[..., 0] = 0\n",
    "    #                 indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "    #                 next_token_logits[indices_to_remove] = float('-inf')\n",
    "                \n",
    "    #             # Sample next token\n",
    "    #             probs = torch.softmax(next_token_logits, dim=-1)\n",
    "    #             next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "    #             # Append next token to decoder input\n",
    "    #             dec_input = torch.cat([dec_input, next_token], dim=1)\n",
    "                \n",
    "    #             # Stop if we predict the END token\n",
    "    #             if (next_token == 2).any():  # Assuming 2 is the END token index\n",
    "    #                 break\n",
    "            \n",
    "    #         return dec_input\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_square_subsequent_mask(sz: int) -> torch.Tensor:\n",
    "        \"\"\"Generate a square mask for the sequence.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd1160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:12<00:00, 12.15s/it]\n",
      "Validating: 100%|██████████| 1/1 [00:00<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 6.5792\n",
      "Validation Loss: 5.7796\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:11<00:00, 11.94s/it]\n",
      "Validating: 100%|██████████| 1/1 [00:00<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Train Loss: 5.4812\n",
      "Validation Loss: 5.5364\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 177\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n",
      "Cell \u001b[0;32mIn[25], line 139\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidating\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 53\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device, pad_idx)\u001b[0m\n\u001b[1;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "from model import MusicTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_src_tgt_data(sequences: torch.Tensor) -> tuple:\n",
    "    \"\"\"Create source and target sequences for training\"\"\"\n",
    "    src = sequences[:, :-1]  # All but last token\n",
    "    tgt = sequences[:, 1:]   # All but first token\n",
    "    return src, tgt\n",
    "\n",
    "def create_padding_mask(seq: torch.Tensor, pad_idx: int = 0) -> torch.Tensor:\n",
    "    \"\"\"Create padding mask for transformer\"\"\"\n",
    "    return (seq == pad_idx)\n",
    "\n",
    "# def train_epoch(\n",
    "#     model: nn.Module,\n",
    "#     dataloader: DataLoader,\n",
    "#     criterion: nn.Module,\n",
    "#     optimizer: optim.Optimizer,\n",
    "#     device: torch.device,\n",
    "#     pad_idx: int = 0\n",
    "# ) -> float:\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for batch_idx, (src, tgt) in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "#         src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "#         # Create masks\n",
    "#         src_padding_mask = create_padding_mask(src, pad_idx).to(device)\n",
    "#         tgt_padding_mask = create_padding_mask(tgt, pad_idx).to(device)\n",
    "#         tgt_mask = model.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(\n",
    "#             src,\n",
    "#             tgt,\n",
    "#             tgt_mask=tgt_mask,\n",
    "#             src_key_padding_mask=src_padding_mask,\n",
    "#             tgt_key_padding_mask=tgt_padding_mask\n",
    "#         )\n",
    "        \n",
    "#         # Calculate loss\n",
    "#         loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "        \n",
    "#     return total_loss / len(dataloader)\n",
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    device: torch.device,\n",
    "    pad_idx: int = 0\n",
    ") -> float:\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, sequences in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        sequences = sequences.to(device)\n",
    "        \n",
    "        # Input and target sequences for autoregressive training\n",
    "        input_seq = sequences[:, :-1]\n",
    "        target_seq = sequences[:, 1:]\n",
    "        \n",
    "        # Create causal mask\n",
    "        seq_len = input_seq.size(1)\n",
    "        mask = model.generate_square_subsequent_mask(seq_len).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq, mask=mask)\n",
    "        \n",
    "        # Calculate loss with diversity regularization\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), target_seq.reshape(-1))\n",
    "        \n",
    "        # Add diversity loss to prevent repetition\n",
    "        diversity_loss = 0\n",
    "        for i in range(1, min(8, seq_len)):\n",
    "            repeated_mask = (input_seq[:, i:] == input_seq[:, :-i]).float()\n",
    "            diversity_loss += repeated_mask.mean()\n",
    "        \n",
    "        total_loss_with_reg = loss + 0.01 * diversity_loss\n",
    "        \n",
    "        total_loss_with_reg.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    device: torch.device,\n",
    "    desc: str,\n",
    "    pad_idx: int = 0\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=desc):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Create masks\n",
    "            src_padding_mask = create_padding_mask(src, pad_idx).to(device)\n",
    "            tgt_padding_mask = create_padding_mask(tgt, pad_idx).to(device)\n",
    "            tgt_mask = model.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(\n",
    "                src,\n",
    "                tgt,\n",
    "                tgt_mask=tgt_mask,\n",
    "                src_key_padding_mask=src_padding_mask,\n",
    "                tgt_key_padding_mask=tgt_padding_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def main():\n",
    "    # Load preprocessed data\n",
    "    print(\"Loading data...\")\n",
    "    datasets = torch.load(\"dataset.pt\")\n",
    "    train_dataset = datasets['train']\n",
    "    val_dataset = datasets['val']\n",
    "    test_dataset = datasets['test']\n",
    "    \n",
    "    with open(\"vocab.pkl\", \"rb\") as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "    \n",
    "    # Create dataloaders for decoder-only training\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Smaller batch size\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MusicTransformer(vocab_size=vocab_data['vocab_size']).to(device)\n",
    "    \n",
    "    # Training parameters\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device, \"Validating\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "            }, 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Load preprocessed data\n",
    "#     print(\"Loading data...\")\n",
    "#     datasets = torch.load(\"dataset.pt\")\n",
    "#     train_dataset = datasets['train']\n",
    "#     val_dataset = datasets['val']\n",
    "#     test_dataset = datasets['test']\n",
    "    \n",
    "#     with open(\"vocab.pkl\", \"rb\") as f:\n",
    "#         vocab_data = pickle.load(f)\n",
    "    \n",
    "#     # Create src-tgt pairs for all sets\n",
    "#     train_src, train_tgt = create_src_tgt_data(train_dataset)\n",
    "#     val_src, val_tgt = create_src_tgt_data(val_dataset)\n",
    "#     test_src, test_tgt = create_src_tgt_data(test_dataset)\n",
    "    \n",
    "#     # Create dataloaders\n",
    "#     train_data = TensorDataset(train_src, train_tgt)\n",
    "#     val_data = TensorDataset(val_src, val_tgt)\n",
    "#     test_data = TensorDataset(test_src, test_tgt)\n",
    "    \n",
    "#     train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "#     val_loader = DataLoader(val_data, batch_size=32)\n",
    "#     test_loader = DataLoader(test_data, batch_size=32)\n",
    "    \n",
    "#     # Initialize model\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = MusicTransformer(vocab_size=vocab_data['vocab_size']).to(device)\n",
    "    \n",
    "#     # Training parameters\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    \n",
    "#     # Training loop\n",
    "#     num_epochs = 50\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience = 10  # Early stopping patience\n",
    "#     patience_counter = 0\n",
    "    \n",
    "#     print(\"Starting training...\")\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Train\n",
    "#         train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "#         # Validate\n",
    "#         val_loss = evaluate(model, val_loader, criterion, device, \"Validating\")\n",
    "        \n",
    "#         # Update learning rate\n",
    "#         scheduler.step(val_loss)\n",
    "        \n",
    "#         # Save best model based on validation loss\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             patience_counter = 0\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'val_loss': val_loss,\n",
    "#             }, 'best_model.pt')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "        \n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#         print(f\"Train Loss: {train_loss:.4f}\")\n",
    "#         print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "#         print(\"------------------------\")\n",
    "        \n",
    "#         # Early stopping\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered!\")\n",
    "#             break\n",
    "    \n",
    "#     # Load best model for final test evaluation\n",
    "#     checkpoint = torch.load('best_model.pt')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     test_loss = evaluate(model, test_loader, criterion, device, \"Testing\")\n",
    "#     print(f\"\\nFinal Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and vocabulary...\n",
      "Vocabulary size: 544\n",
      "Special tokens: ['[PAD]', '[START]', '[END]', '[UNK]']\n",
      "Using device: cpu\n",
      "Loading model weights...\n",
      "\n",
      "Generating music samples...\n",
      "\n",
      "Generating sample 1/1\n",
      "\n",
      "Converting MIDI to audio...\n",
      "Converting generated_music_0.mid to generated_music_0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fluidsynth: warning: Sample 'SineWave': ROM sample ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated files:FluidSynth runtime version 2.4.6\n",
      "Copyright (C) 2000-2025 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of Creative Technology Ltd.\n",
      "\n",
      "Rendering audio to file 'generated_music_0.wav'..\n",
      "\n",
      "Sample 1:\n",
      "  MIDI: generated_music_0.mid\n",
      "  WAV: generated_music_0.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from model import MusicTransformer\n",
    "from music21 import note, chord, stream, instrument\n",
    "from typing import List, Union\n",
    "from fractions import Fraction\n",
    "from midi2audio import FluidSynth\n",
    "import os\n",
    "\n",
    "def parse_duration(duration_str: str) -> float:\n",
    "    \"\"\"Parse duration string that might be a fraction or float\"\"\"\n",
    "    try:\n",
    "        if '/' in duration_str:\n",
    "            return float(Fraction(duration_str))\n",
    "        return float(duration_str)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error parsing duration {duration_str}: {e}\")\n",
    "        return 1.0  # Default duration\n",
    "\n",
    "def token_to_music21(token: str) -> Union[note.Note, chord.Chord, None]:\n",
    "    \"\"\"Convert a token to a music21 note or chord\"\"\"\n",
    "    try:\n",
    "        if token.startswith('NOTE_'):\n",
    "            _, pitch, duration = token.split('_')\n",
    "            if not pitch:\n",
    "                print(f\"Invalid pitch in token: {token}\")\n",
    "                return None\n",
    "            return note.Note(pitch, quarterLength=parse_duration(duration))\n",
    "        elif token.startswith('CHORD_'):\n",
    "            parts = token.split('_')\n",
    "            if len(parts) != 3:\n",
    "                print(f\"Invalid chord token format: {token}\")\n",
    "                return None\n",
    "            _, pitches_str, duration = parts\n",
    "            if not pitches_str:\n",
    "                print(f\"Empty pitches in chord token: {token}\")\n",
    "                return None\n",
    "            \n",
    "            # Split and clean up pitch names\n",
    "            pitches = []\n",
    "            raw_pitches = pitches_str.split('-')\n",
    "            i = 0\n",
    "            while i < len(raw_pitches):\n",
    "                p = raw_pitches[i]\n",
    "                if not p:\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # If this pitch has a number, it's complete\n",
    "                if any(c.isdigit() for c in p):\n",
    "                    pitches.append(p)\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                # If this pitch doesn't have a number but the next one does\n",
    "                if i + 1 < len(raw_pitches) and any(c.isdigit() for c in raw_pitches[i + 1]):\n",
    "                    pitches.append(p + raw_pitches[i + 1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    # Use the octave from the previous note or default to 4\n",
    "                    prev_octave = ''.join(c for c in pitches[-1] if c.isdigit()) if pitches else '4'\n",
    "                    pitches.append(p + prev_octave)\n",
    "                    i += 1\n",
    "            \n",
    "            if not pitches:\n",
    "                print(f\"No valid pitches in chord token: {token}\")\n",
    "                return None\n",
    "                \n",
    "            return chord.Chord(pitches, quarterLength=parse_duration(duration))\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting token {token}: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_music(\n",
    "    model: MusicTransformer,\n",
    "    vocab_data: dict,\n",
    "    seed_sequence: torch.Tensor = None,\n",
    "    max_len: int = 512,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    "    min_sequence_length: int = 64,  # Minimum sequence length to ensure enough music\n",
    "    device: torch.device = None\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate a sequence of music tokens\"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    try:\n",
    "        # If no seed sequence provided, start with just the START token\n",
    "        if seed_sequence is None:\n",
    "            # Find the START token index\n",
    "            start_token = '[START]'\n",
    "            token2idx = vocab_data['token2idx']\n",
    "            start_idx = token2idx.get(start_token, 1)  # Default to 1 if not found\n",
    "            seed_sequence = torch.tensor([[start_idx]]).to(device)\n",
    "        \n",
    "        # Generate sequence\n",
    "        generated_sequence = model.generate(\n",
    "            seed_sequence,\n",
    "            max_len=max_len,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        \n",
    "        # Convert indices to tokens\n",
    "        idx2token = vocab_data['idx2token']\n",
    "        tokens = []\n",
    "        for idx in generated_sequence[0]:\n",
    "            idx_num = idx.item()\n",
    "            token = idx2token.get(idx_num, '[UNK]')\n",
    "            \n",
    "            # Skip special tokens except START and END\n",
    "            if token in ['[PAD]', '[UNK]']:\n",
    "                continue\n",
    "                \n",
    "            tokens.append(token)\n",
    "            \n",
    "            # If we have enough tokens and encounter END, stop\n",
    "            if token == '[END]' and len(tokens) >= min_sequence_length:\n",
    "                break\n",
    "        \n",
    "        # If we don't have enough tokens or no END token was found, generate more\n",
    "        while len(tokens) < min_sequence_length:\n",
    "            # Generate more sequence\n",
    "            additional_sequence = model.generate(\n",
    "                torch.tensor([[token2idx[tokens[-1]]]]).to(device),\n",
    "                max_len=32,  # Generate a shorter sequence\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p\n",
    "            )\n",
    "            \n",
    "            # Add new tokens\n",
    "            for idx in additional_sequence[0][1:]:  # Skip first token as it's the seed\n",
    "                idx_num = idx.item()\n",
    "                token = idx2token.get(idx_num, '[UNK]')\n",
    "                if token not in ['[PAD]', '[UNK]', '[START]']:\n",
    "                    tokens.append(token)\n",
    "                    if token == '[END]' and len(tokens) >= min_sequence_length:\n",
    "                        break\n",
    "        \n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_music: {e}\")\n",
    "        print(f\"Vocabulary structure: {vocab_data.keys()}\")\n",
    "        print(f\"idx2token keys type: {type(next(iter(idx2token.keys())))} example: {next(iter(idx2token.keys()))}\")\n",
    "        raise\n",
    "\n",
    "def tokens_to_music21(tokens: List[str]) -> stream.Stream:\n",
    "    \"\"\"Convert a list of tokens to a music21 stream\"\"\"\n",
    "    score = stream.Score()\n",
    "    part = stream.Part()\n",
    "    \n",
    "    # Set the instrument to Violin\n",
    "    violin = instrument.Violin()\n",
    "    part.insert(0, violin)\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.startswith(('[START]', '[END]', '[PAD]', '[UNK]')):\n",
    "            continue\n",
    "        \n",
    "        element = token_to_music21(token)\n",
    "        if element is not None:\n",
    "            part.append(element)\n",
    "    \n",
    "    score.append(part)\n",
    "    return score\n",
    "\n",
    "def generate_multiple_samples(model, vocab_data, num_samples=5, **kwargs):\n",
    "    \"\"\"Generate multiple music samples\"\"\"\n",
    "    midi_files = []\n",
    "    wav_files = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        print(f\"\\nGenerating sample {i+1}/{num_samples}\")\n",
    "        \n",
    "        # Generate tokens\n",
    "        tokens = generate_music(model, vocab_data, **kwargs)\n",
    "        \n",
    "        # Convert to score\n",
    "        score = tokens_to_music21(tokens)\n",
    "        \n",
    "        # Save as MIDI\n",
    "        midi_file = f\"generated_music_{i}.mid\"\n",
    "        score.write('midi', fp=midi_file)\n",
    "        midi_files.append(midi_file)\n",
    "        \n",
    "        # Convert to WAV\n",
    "        wav_file = f\"generated_music_{i}.wav\"\n",
    "        wav_files.append(wav_file)\n",
    "    \n",
    "    return midi_files, wav_files\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load vocabulary and model\n",
    "        print(\"Loading model and vocabulary...\")\n",
    "        with open(\"vocab.pkl\", \"rb\") as f:\n",
    "            vocab_data = pickle.load(f)\n",
    "        \n",
    "        # Print vocabulary information\n",
    "        print(f\"Vocabulary size: {vocab_data['vocab_size']}\")\n",
    "        print(f\"Special tokens: {[token for token in vocab_data['token2idx'].keys() if token.startswith('[')]}\")\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        model = MusicTransformer(vocab_size=vocab_data['vocab_size']).to(device)\n",
    "        \n",
    "        # Load trained model weights\n",
    "        print(\"Loading model weights...\")\n",
    "        checkpoint = torch.load(\"best_model.pt\", map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Generate multiple samples\n",
    "        print(\"\\nGenerating music samples...\")\n",
    "        num_samples = 1\n",
    "        midi_files, wav_files = generate_multiple_samples(\n",
    "            model,\n",
    "            vocab_data,\n",
    "            num_samples=num_samples,\n",
    "            temperature=0.85,  # Higher temperature for more variety\n",
    "            top_k=40,         # Less restrictive filtering\n",
    "            top_p=0.92,       # Less restrictive nucleus sampling\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Convert MIDI to audio using FluidSynth\n",
    "        print(\"\\nConverting MIDI to audio...\")\n",
    "        try:\n",
    "            fs = FluidSynth(\"default.sf2\")  # Using the default soundfont that works\n",
    "            for midi_file, wav_file in zip(midi_files, wav_files):\n",
    "                print(f\"Converting {midi_file} to {wav_file}\")\n",
    "                fs.midi_to_audio(midi_file, wav_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to audio: {e}\")\n",
    "            print(\"Make sure the soundfont file is present in the current directory\")\n",
    "        \n",
    "        print(\"\\nGenerated files:\")\n",
    "        for i in range(num_samples):\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            print(f\"  MIDI: {midi_files[i]}\")\n",
    "            print(f\"  WAV: {wav_files[i]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main: {e}\")\n",
    "        print(\"Vocabulary data keys:\", vocab_data.keys() if 'vocab_data' in locals() else \"vocab_data not loaded\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16336d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
