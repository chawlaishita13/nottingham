{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ce1b33b",
   "metadata": {},
   "source": [
    "# 🎼 Task 2: Conditioned Symbolic Music Generation with LSTM\n",
    "This notebook extends the Task 1 LSTM-based symbolic music generator by conditioning generation on chord tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa023f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting music21\n",
      "  Downloading music21-8.3.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting chardet (from music21)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.9/site-packages (from music21) (1.5.1)\n",
      "Collecting jsonpickle (from music21)\n",
      "  Downloading jsonpickle-4.1.1-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (from music21) (3.9.4)\n",
      "Collecting more-itertools (from music21)\n",
      "  Using cached more_itertools-10.7.0-py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.9/site-packages (from music21) (2.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from music21) (2.32.3)\n",
      "Collecting webcolors>=1.5 (from music21)\n",
      "  Using cached webcolors-24.11.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./venv/lib/python3.9/site-packages (from matplotlib->music21) (6.5.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->music21) (3.22.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->music21) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests->music21) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests->music21) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->music21) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.9/site-packages (from requests->music21) (2025.4.26)\n",
      "Downloading music21-8.3.0-py3-none-any.whl (22.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.8/22.8 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached webcolors-24.11.1-py3-none-any.whl (14 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading jsonpickle-4.1.1-py3-none-any.whl (47 kB)\n",
      "Using cached more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "Installing collected packages: webcolors, more-itertools, jsonpickle, chardet, music21\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [music21]m4/5\u001b[0m [music21]\n",
      "\u001b[1A\u001b[2KSuccessfully installed chardet-5.2.0 jsonpickle-4.1.1 more-itertools-10.7.0 music21-8.3.0 webcolors-24.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install music21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89208a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishetabansal/Documents/CSE253/Assignment2/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from music21 import converter, note, chord, stream, instrument\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedcc5c0",
   "metadata": {},
   "source": [
    "## 🧩 Helper Class for Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa913ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedMidiLSTM:\n",
    "    def __init__(self):\n",
    "        self.note_to_int = {}\n",
    "        self.int_to_note = {}\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def parse_midi(self, file_path):\n",
    "        midi = converter.parse(file_path)\n",
    "        notes = []\n",
    "        parts = instrument.partitionByInstrument(midi)\n",
    "\n",
    "        if parts:  # file has instrument parts\n",
    "            for element in parts.parts[0].recurse():\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "        return notes\n",
    "\n",
    "    def preprocess_midi_files(self, directory, max_files=None):\n",
    "        all_notes = []\n",
    "        files = list(Path(directory).rglob(\"*.mid\"))[:max_files]\n",
    "        for file in files:\n",
    "            notes = self.parse_midi(file)\n",
    "            if len(notes) > 0:\n",
    "                # Fake chord condition (could be improved)\n",
    "                chord_token = random.choice(['C', 'G', 'Am', 'F'])  \n",
    "                all_notes.extend([chord_token] + notes)\n",
    "        return all_notes\n",
    "\n",
    "    def create_vocabulary(self, notes):\n",
    "        unique_notes = sorted(set(notes))\n",
    "        self.note_to_int = {note: i for i, note in enumerate(unique_notes)}\n",
    "        self.int_to_note = {i: note for note, i in self.note_to_int.items()}\n",
    "        self.vocab_size = len(unique_notes)\n",
    "        return notes\n",
    "\n",
    "    def create_sequences(self, notes, seq_length=50):\n",
    "        inputs, targets = [], []\n",
    "        for i in range(len(notes) - seq_length):\n",
    "            seq_in = notes[i:i + seq_length]\n",
    "            seq_out = notes[i + seq_length]\n",
    "            inputs.append([self.note_to_int[n] for n in seq_in])\n",
    "            targets.append(self.note_to_int[seq_out])\n",
    "        return np.array(inputs), to_categorical(targets, num_classes=self.vocab_size)\n",
    "\n",
    "    def build_model(self, seq_length):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=self.vocab_size, output_dim=100, input_length=seq_length))\n",
    "        model.add(LSTM(256, return_sequences=True))\n",
    "        model.add(LSTM(256))\n",
    "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        return model\n",
    "\n",
    "    def sample(self, preds, temperature=1.0):\n",
    "        preds = np.log(preds + 1e-9) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        return np.random.choice(range(len(preds)), p=preds)\n",
    "\n",
    "    def generate(self, model, seed_seq, length=100):\n",
    "        result = []\n",
    "        current_seq = seed_seq.copy()\n",
    "        for _ in range(length):\n",
    "            prediction = model.predict(np.array([current_seq]), verbose=0)[0]\n",
    "            index = self.sample(prediction, temperature=0.9)\n",
    "            result.append(index)\n",
    "            current_seq = current_seq[1:] + [index]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434da6a9",
   "metadata": {},
   "source": [
    "## 🚀 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b923e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishetabansal/Documents/CSE253/Assignment2/venv/lib/python3.9/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 115ms/step - loss: 3.4237 - val_loss: 3.0625\n",
      "Epoch 2/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 149ms/step - loss: 3.0039 - val_loss: 2.8245\n",
      "Epoch 3/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 148ms/step - loss: 2.8187 - val_loss: 2.6719\n",
      "Epoch 4/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 143ms/step - loss: 2.6765 - val_loss: 2.5610\n",
      "Epoch 5/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 145ms/step - loss: 2.5500 - val_loss: 2.4663\n",
      "Epoch 6/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 142ms/step - loss: 2.4137 - val_loss: 2.3823\n",
      "Epoch 7/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 157ms/step - loss: 2.2537 - val_loss: 2.2259\n",
      "Epoch 8/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 153ms/step - loss: 2.1112 - val_loss: 2.1022\n",
      "Epoch 9/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 155ms/step - loss: 1.9172 - val_loss: 1.9708\n",
      "Epoch 10/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 147ms/step - loss: 1.7215 - val_loss: 1.8448\n",
      "Epoch 11/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 129ms/step - loss: 1.5178 - val_loss: 1.7447\n",
      "Epoch 12/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 138ms/step - loss: 1.3166 - val_loss: 1.6401\n",
      "Epoch 13/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 126ms/step - loss: 1.0870 - val_loss: 1.5822\n",
      "Epoch 14/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 117ms/step - loss: 0.9104 - val_loss: 1.4717\n",
      "Epoch 15/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 124ms/step - loss: 0.7533 - val_loss: 1.3888\n",
      "Epoch 16/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 115ms/step - loss: 0.6157 - val_loss: 1.3766\n",
      "Epoch 17/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.5193 - val_loss: 1.3003\n",
      "Epoch 18/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 123ms/step - loss: 0.4051 - val_loss: 1.2604\n",
      "Epoch 19/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 129ms/step - loss: 0.3417 - val_loss: 1.2164\n",
      "Epoch 20/20\n",
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 130ms/step - loss: 0.2712 - val_loss: 1.2363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "midi_lstm = ConditionedMidiLSTM()\n",
    "\n",
    "# Load and process data\n",
    "notes = midi_lstm.preprocess_midi_files(\"nottingham-dataset/MIDI\", max_files=30)\n",
    "filtered_notes = midi_lstm.create_vocabulary(notes)\n",
    "X, y = midi_lstm.create_sequences(filtered_notes, seq_length=50)\n",
    "\n",
    "# Split and train\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "model = midi_lstm.build_model(seq_length=50)\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)\n",
    "model.save(\"conditioned_lstm_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bea3d5",
   "metadata": {},
   "source": [
    "## 🎼 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb283c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved seed input MIDI as 'task2_seed_input.mid'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'task2_generated.mid'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from music21 import stream, note, chord, instrument\n",
    "\n",
    "# Pick a random seed sequence with a chord token at start\n",
    "start = random.randint(0, len(X) - 1)\n",
    "seed = list(X_val[start])\n",
    "\n",
    "# Save the random seed sequence used for generation\n",
    "seed_stream = stream.Stream()\n",
    "seed_stream.append(instrument.Piano())\n",
    "\n",
    "for idx in seed:\n",
    "    token = midi_lstm.int_to_note[idx]\n",
    "    if '.' in token:  # Chord\n",
    "        seed_stream.append(chord.Chord([int(n) for n in token.split('.')]))\n",
    "    else:  # Single note\n",
    "        seed_stream.append(note.Note(token))\n",
    "\n",
    "# Save to MIDI\n",
    "seed_stream.write(\"midi\", fp=\"task2_seed_input.mid\")\n",
    "print(\"Saved seed input MIDI as 'task2_seed_input.mid'\")\n",
    "\n",
    "# Generate indices\n",
    "generated = midi_lstm.generate(model, seed, length=100)\n",
    "generated_notes = [midi_lstm.int_to_note[idx] for idx in generated]\n",
    "\n",
    "# Convert to MIDI\n",
    "output_stream = stream.Stream()\n",
    "output_stream.append(instrument.Piano())\n",
    "for token in generated_notes:\n",
    "    if '.' in token:\n",
    "        output_stream.append(chord.Chord([int(n) for n in token.split('.')]))\n",
    "    else:\n",
    "        output_stream.append(note.Note(token))\n",
    "output_stream.write(\"midi\", fp=\"task2_generated.mid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159449b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
